<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="twitter:card" content="summary_large_image" /><!-- Begin Jekyll SEO tag v2.6.1 -->
<title>Fraud Detection Tabular Kaggle competition | My DS Blog</title>
<meta name="generator" content="Jekyll v4.1.1" />
<meta property="og:title" content="Fraud Detection Tabular Kaggle competition" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="My attempt to get to the top 10% LB" />
<meta property="og:description" content="My attempt to get to the top 10% LB" />
<link rel="canonical" href="https://tkravichandran.github.io/my-fast-ds-blog/first-tabular-kaggle-competition.html" />
<meta property="og:url" content="https://tkravichandran.github.io/my-fast-ds-blog/first-tabular-kaggle-competition.html" />
<meta property="og:site_name" content="My DS Blog" />
<meta property="og:image" content="https://tkravichandran.github.io/my-fast-ds-blog/images/fraud-detection/chart.png" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2021-02-18T00:00:00-06:00" />
<script type="application/ld+json">
{"headline":"Fraud Detection Tabular Kaggle competition","dateModified":"2021-02-18T00:00:00-06:00","url":"https://tkravichandran.github.io/my-fast-ds-blog/first-tabular-kaggle-competition.html","datePublished":"2021-02-18T00:00:00-06:00","@type":"BlogPosting","image":"https://tkravichandran.github.io/my-fast-ds-blog/images/fraud-detection/chart.png","mainEntityOfPage":{"@type":"WebPage","@id":"https://tkravichandran.github.io/my-fast-ds-blog/first-tabular-kaggle-competition.html"},"description":"My attempt to get to the top 10% LB","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/my-fast-ds-blog/assets/css/style.css"><link type="application/atom+xml" rel="alternate" href="https://tkravichandran.github.io/my-fast-ds-blog/feed.xml" title="My DS Blog" /><link rel="shortcut icon" type="image/x-icon" href="/my-fast-ds-blog/images/favicon3.ico">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/Primer/15.2.0/primer.css" integrity="sha512-xTz2ys4coGAOz8vuV1NcQBkgVmKhsSEtjbqyMJbBHRplFuvKIUo6xhLHpAyPt9mfR6twHJgn9OgVLuqOvjeBhg==" crossorigin="anonymous" />
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css" integrity="sha512-1PKOgIY59xJ8Co8+NE6FZ+LOAZKjy+KY8iq0G4B3CyeY6wYHN3yt9PW0XpSriVlkMXe40PTKnXrLnZ9+fkDaog==" crossorigin="anonymous" />

<script>
function wrap_img(fn) {
    if (document.attachEvent ? document.readyState === "complete" : document.readyState !== "loading") {
        var elements = document.querySelectorAll(".post img");
        Array.prototype.forEach.call(elements, function(el, i) {
            if (el.getAttribute("title") && (el.className != "emoji")) {
                const caption = document.createElement('figcaption');
                var node = document.createTextNode(el.getAttribute("title"));
                caption.appendChild(node);
                const wrapper = document.createElement('figure');
                wrapper.className = 'image';
                el.parentNode.insertBefore(wrapper, el);
                el.parentNode.removeChild(el);
                wrapper.appendChild(el);
                wrapper.appendChild(caption);
            }
        });
    } else { document.addEventListener('DOMContentLoaded', fn); }
}
window.onload = wrap_img;
</script>

<script>
    document.addEventListener("DOMContentLoaded", function(){
    // add link icon to anchor tags
    var elem = document.querySelectorAll(".anchor-link")
    elem.forEach(e => (e.innerHTML = '<i class="fas fa-link fa-xs"></i>'));
    });
</script>
</head>
<body><header class="site-header">

  <div class="wrapper"><a class="site-title" rel="author" href="/my-fast-ds-blog/">My DS Blog</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/my-fast-ds-blog/:title:.html">About</a><a class="page-link" href="/my-fast-ds-blog/_pages/archive.html">Archive</a><a class="page-link" href="/my-fast-ds-blog/_pages/notes.html">Notes</a><a class="page-link" href="/my-fast-ds-blog/search/">Search</a><a class="page-link" href="/my-fast-ds-blog/categories/">Tags</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Fraud Detection Tabular Kaggle competition</h1><p class="page-description">My attempt to get to the top 10% LB</p><p class="post-meta post-meta-title"><time class="dt-published" datetime="2021-02-18T00:00:00-06:00" itemprop="datePublished">
        Feb 18, 2021
      </time>
       • <span class="read-time" title="Estimated read time">
    
    
      19 min read
    
</span></p>

    
      <p class="category-tags"><i class="fas fa-tags category-tags-icon"></i></i> 
      
        <a class="category-tags-link" href="/my-fast-ds-blog/categories/#markdown">markdown</a>
        &nbsp;
      
        <a class="category-tags-link" href="/my-fast-ds-blog/categories/#posts">posts</a>
        
      
      </p>
    

    </header>

  <div class="post-content e-content" itemprop="articleBody">
    <ul class="section-nav">
<li class="toc-entry toc-h2"><a href="#introduction">Introduction</a></li>
<li class="toc-entry toc-h2"><a href="#columns-explanation">Columns explanation</a></li>
<li class="toc-entry toc-h2"><a href="#tldr">TL;DR</a></li>
<li class="toc-entry toc-h2"><a href="#the-part-that-makes-it-hard-to-predict">The Part that makes it hard to predict</a></li>
<li class="toc-entry toc-h2"><a href="#way-of-work">Way of work</a></li>
<li class="toc-entry toc-h1"><a href="#eda">EDA</a>
<ul>
<li class="toc-entry toc-h2"><a href="#lots-of-nans">Lots of NaNs</a></li>
<li class="toc-entry toc-h2"><a href="#reducing-339-v-columns-to-139-v-columns">Reducing 339 V columns to 139 V columns</a></li>
<li class="toc-entry toc-h2"><a href="#reducing-further">Reducing further</a></li>
<li class="toc-entry toc-h2"><a href="#understanding-d-columns">Understanding D columns</a></li>
<li class="toc-entry toc-h2"><a href="#unbalanced-data">Unbalanced data</a></li>
<li class="toc-entry toc-h2"><a href="#eda-for-uid">EDA for UID</a></li>
</ul>
</li>
<li class="toc-entry toc-h1"><a href="#model">Model</a></li>
<li class="toc-entry toc-h1"><a href="#feature-engineering">Feature engineering</a></li>
<li class="toc-entry toc-h1"><a href="#post-process">Post Process</a></li>
<li class="toc-entry toc-h1"><a href="#local-validation-and-prediction-strategy">Local Validation and Prediction Strategy</a></li>
<li class="toc-entry toc-h1"><a href="#memory-reduction">Memory reduction</a></li>
<li class="toc-entry toc-h1"><a href="#practical-understanding-of-overfitting-and-av">Practical understanding of Overfitting and AV</a>
<ul>
<li class="toc-entry toc-h2"><a href="#preventing-overfitting">Preventing overfitting</a></li>
<li class="toc-entry toc-h2"><a href="#what-can-your-av-do-for-you">What can your AV do for you?</a></li>
<li class="toc-entry toc-h2"><a href="#references">References</a></li>
</ul>
</li>
</ul><h2 id="introduction">
<a class="anchor" href="#introduction" aria-hidden="true"><span class="octicon octicon-link"></span></a><strong>Introduction</strong>
</h2>

<ul>
  <li>Tabular time series data with 590k train transactions and 500k test
transactions.</li>
  <li>443 columns as independent variables</li>
  <li>“isFraud” is the dependent variable</li>
  <li>Leader Board (LB) is based on the public and private AUC score.</li>
</ul>

<p><strong>Goal</strong>: Predict transactions that are <code class="language-plaintext highlighter-rouge">isFraud==1</code>.</p>

<p><strong>Submission</strong>: CSV file with <code class="language-plaintext highlighter-rouge">TransactionID</code> and Probability of
<code class="language-plaintext highlighter-rouge">isFraud==1</code>.</p>

<p><strong>Grading</strong>: Based on AUC score.</p>

<p>Note: This is a year old competition that wont doesn’t allow “final
submissions” anymore. Nevertheless I picked this competition to get
experience in Tabular data, attempt my best solutions and learn from
the greats.</p>

<p>Following is my systematic growth to a top 10% solution.</p>

<h2 id="columns-explanation">
<a class="anchor" href="#columns-explanation" aria-hidden="true"><span class="octicon octicon-link"></span></a><strong>Columns explanation</strong>
</h2>

<ul>
  <li>
<code class="language-plaintext highlighter-rouge">TransactionDT</code>: timedelta from a given reference datetime (not an actual timestamp)</li>
  <li>
<code class="language-plaintext highlighter-rouge">TransactionAMT</code>: transaction payment amount in USD</li>
  <li>
<code class="language-plaintext highlighter-rouge">ProductCD</code>: product code, the product for each transaction</li>
  <li>
<code class="language-plaintext highlighter-rouge">card1</code> - <code class="language-plaintext highlighter-rouge">card6</code>: payment card information, such as card type, card category, issue bank, country, etc.</li>
  <li>
<code class="language-plaintext highlighter-rouge">addr</code>: address</li>
  <li>
<code class="language-plaintext highlighter-rouge">dist</code>: distance</li>
  <li>
<code class="language-plaintext highlighter-rouge">P_ and (R__) emaildomain</code>: purchaser and recipient email domain</li>
  <li>
<code class="language-plaintext highlighter-rouge">C1-C14</code>: counting, such as how many addresses are found to be
associated with the payment card, etc. The actual meaning is masked.</li>
  <li>
<code class="language-plaintext highlighter-rouge">D1-D15:</code> timedelta, such as days between previous transaction, etc.</li>
  <li>
<code class="language-plaintext highlighter-rouge">M1-M9</code>: match, such as names on card and address, etc.</li>
  <li>
<code class="language-plaintext highlighter-rouge">Vxxx</code>: Vesta engineered rich features, including ranking, counting, and
other entity relations.</li>
</ul>

<p><strong>Categorical Features:</strong></p>
<ul>
  <li><code class="language-plaintext highlighter-rouge">ProductCD</code></li>
  <li><code class="language-plaintext highlighter-rouge">card1 - card6</code></li>
  <li>
<code class="language-plaintext highlighter-rouge">addr1</code>, <code class="language-plaintext highlighter-rouge">addr2</code>
</li>
  <li><code class="language-plaintext highlighter-rouge">P_emaildomain</code></li>
  <li><code class="language-plaintext highlighter-rouge">R_emaildomain</code></li>
  <li><code class="language-plaintext highlighter-rouge">M1 - M9</code></li>
  <li><code class="language-plaintext highlighter-rouge">id_12 - id_38</code></li>
</ul>

<hr>

<h2 id="tldr">
<a class="anchor" href="#tldr" aria-hidden="true"><span class="octicon octicon-link"></span></a><strong>TL;DR</strong>
</h2>

<p>My final Kaggle Kernel is here:</p>

<p>The following are roughly the things that improved my score.</p>

<table>
  <thead>
    <tr>
      <th>Method</th>
      <th>Public LB</th>
      <th>Private LB</th>
      <th>Percentile</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Baseline  XGB</td>
      <td>0.9384</td>
      <td>0.9096</td>
      <td>Top 90%</td>
    </tr>
    <tr>
      <td>Remove 200 <code class="language-plaintext highlighter-rouge">V</code>
</td>
      <td>0.9377</td>
      <td>0.9107</td>
      <td>Top 80%</td>
    </tr>
    <tr>
      <td>Remove time cols</td>
      <td>0.9374</td>
      <td>0.9109</td>
      <td>Top 80%</td>
    </tr>
    <tr>
      <td>Transform <code class="language-plaintext highlighter-rouge">D</code>
</td>
      <td>0.9429</td>
      <td>0.9117</td>
      <td>Top 50%</td>
    </tr>
    <tr>
      <td>Combine cols and Frequency Encoding</td>
      <td>0.9471</td>
      <td>0.9146</td>
      <td>Top 30%</td>
    </tr>
    <tr>
      <td>Aggregation on uid1</td>
      <td>0.9513</td>
      <td>0.9203</td>
      <td>Top 27%</td>
    </tr>
    <tr>
      <td>Additional aggregations</td>
      <td>0.9535</td>
      <td>0.9220</td>
      <td>Top 5%</td>
    </tr>
    <tr>
      <td>Fillna</td>
      <td>0.9537</td>
      <td>0.9223</td>
      <td>Top 5%</td>
    </tr>
    <tr>
      <td>Changed UID</td>
      <td>0.9543</td>
      <td>0.9264</td>
      <td>Top 2%</td>
    </tr>
  </tbody>
</table>

<p>Main kernel of work done is <a href="https://www.kaggle.com/thejravichandran/fraud-detection-v16-new-pipleline-and-testing/notebook">here</a>.</p>

<p><strong>AUC score increase over time:</strong></p>

<p><img src="./images/fraud-detection/chart.png" alt="chart.png"></p>

<hr>

<h2 id="the-part-that-makes-it-hard-to-predict">
<a class="anchor" href="#the-part-that-makes-it-hard-to-predict" aria-hidden="true"><span class="octicon octicon-link"></span></a><strong>The Part that makes it hard to predict</strong>
</h2>

<ul>
  <li>
    <p>We are <strong>not predicting Fraudulent Transactions</strong>. Once a client has
a fraudulent transaction <a href="https://www.kaggle.com/c/ieee-fraud-detection/discussion/101203">all the posterior transactions (associated
with the useraccount, email address or billing address) are marked
as fraud</a>.</p>

    <p>And it is not known as to <strong>What constitutes the client</strong>. Below is
an example of an “assumed client-variable set”. Such a client could
potentially have both <code class="language-plaintext highlighter-rouge">isFraud=0</code> and <code class="language-plaintext highlighter-rouge">isFraud=1</code> in their
transactions, making it harder to detect clients with fraudulent
transactions.</p>
  </li>
</ul>

<p><img src="./images/fraud-detection/client-isFraud1.png" alt="client-isFraud1.png"></p>

<ul>
  <li>To make this even more difficult the meaning of <strong>most of the
columns are obscured</strong>. For example, we know that <code class="language-plaintext highlighter-rouge">D1</code> to <code class="language-plaintext highlighter-rouge">D15</code> are
some “timedeltas” such as days between each transaction, but don’t
know what exactly they stand for. This goes for all 439 columns.</li>
</ul>

<h2 id="way-of-work">
<a class="anchor" href="#way-of-work" aria-hidden="true"><span class="octicon octicon-link"></span></a><strong>Way of work</strong>
</h2>

<ol>
  <li>
    <p>Start with building a quick model after having a quick look at the
data.</p>
  </li>
  <li>
    <p>Do detailed EDA and test different ideas on the quick model.</p>
  </li>
  <li>
    <p>submit model to get test score and iterate.</p>
  </li>
</ol>

<hr>

<h1 id="eda">
<a class="anchor" href="#eda" aria-hidden="true"><span class="octicon octicon-link"></span></a>EDA</h1>

<h2 id="lots-of-nans">
<a class="anchor" href="#lots-of-nans" aria-hidden="true"><span class="octicon octicon-link"></span></a><strong>Lots of NaNs</strong>
</h2>

<ul>
  <li>
    <p>The data has 254 variables (out of 439) with &gt;25% NaNs.</p>
  </li>
  <li>
    <p>I tried <code class="language-plaintext highlighter-rouge">fillna</code>, removing the columns completely and also just
leaving them as NANs.</p>
  </li>
  <li>
    <p>I found best results with filling NaNs with an artificial number
such as <code class="language-plaintext highlighter-rouge">-9999</code>. Removing <code class="language-plaintext highlighter-rouge">&gt;25%</code> NaN columns seems to remove
critical information. Results with <code class="language-plaintext highlighter-rouge">fillna</code> were better by 0.0002
than leaving NaNs as it is.</p>
  </li>
</ul>

<p><strong>Median Imputing doesn’t make sense</strong></p>

<p>There are columns such as “card1” and “addr1” which denote credit card
and address information respectively, but there is no way a median
imputation makes any sense. So this is not done.</p>

<h2 id="reducing-339-v-columns-to-139-v-columns">
<a class="anchor" href="#reducing-339-v-columns-to-139-v-columns" aria-hidden="true"><span class="octicon octicon-link"></span></a><strong>Reducing 339 <code class="language-plaintext highlighter-rouge">V</code> columns to 139 <code class="language-plaintext highlighter-rouge">V</code> columns</strong>
</h2>

<p>There were about 340 <code class="language-plaintext highlighter-rouge">V</code> columns said to be “engineered” by the
company conducting the competition. They are engineered from the other
100 columns.</p>

<p>The <code class="language-plaintext highlighter-rouge">V</code> columns share a “lot” of correlation and a large number of
Nan’s. The goal of this step is to find “similar” columns based on
number of “NaN’s”, and Correlation&gt;<code class="language-plaintext highlighter-rouge">0.75</code>. This process is automated
for all the different <code class="language-plaintext highlighter-rouge">NaN</code> groups, using a script. The method is
explained as below:</p>

<ul>
  <li>First group by NaN’s</li>
  <li>Within each Nan group, highly correlated columns are binned together</li>
  <li>The column with maximum unique values is chosen from each bin.</li>
</ul>

<p>For example, <code class="language-plaintext highlighter-rouge">V35</code> to <code class="language-plaintext highlighter-rouge">V52</code> (17 columns) contain the exact same number
of NaNs (168k). They contain 8 pairs of highly correlated columns as
shown below. From this we select <code class="language-plaintext highlighter-rouge">['V36', 'V44', 'V39', 'V49', 'V47',
'V41', 'V40', 'V38']</code> (8 columns).</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">plt</span><span class="p">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">15</span><span class="p">,</span><span class="mi">15</span><span class="p">))</span>
<span class="c1">#mask = np.triu(xs_corr)
</span><span class="n">sns</span><span class="p">.</span><span class="n">heatmap</span><span class="p">(</span><span class="n">xs_corr</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s">'RdBu_r'</span><span class="p">,</span> <span class="n">annot</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="n">center</span><span class="o">=</span><span class="mf">0.0</span><span class="p">)</span><span class="c1">#, mask=mask)
</span><span class="n">plt</span><span class="p">.</span><span class="n">title</span><span class="p">(</span><span class="s">'All Cols'</span><span class="p">,</span><span class="n">fontsize</span><span class="o">=</span><span class="mi">14</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div></div>

<p><img src="./images/fraud-detection/corr.png" alt="corr.png"></p>

<p><strong>Effect on LB and time:</strong> Small decrease in score for a large gain in
computation time.</p>

<table>
  <thead>
    <tr>
      <th> </th>
      <th>public</th>
      <th>private</th>
      <th>time</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>baseline</td>
      <td>0.9384</td>
      <td>0.9096</td>
      <td>11mins</td>
    </tr>
    <tr>
      <td>remove 200 V columns</td>
      <td>0.9377</td>
      <td>0.9107</td>
      <td>7mins</td>
    </tr>
  </tbody>
</table>

<h2 id="reducing-further">
<a class="anchor" href="#reducing-further" aria-hidden="true"><span class="octicon octicon-link"></span></a><strong>Reducing further</strong>
</h2>

<p>Tried reducing other columns, such as the <code class="language-plaintext highlighter-rouge">C</code>, <code class="language-plaintext highlighter-rouge">M</code> and <code class="language-plaintext highlighter-rouge">ID</code> columns in
a same manner but then they reduced the LB by 0.01, so this is as far
as the reduction goes.</p>

<h2 id="understanding-d-columns">
<a class="anchor" href="#understanding-d-columns" aria-hidden="true"><span class="octicon octicon-link"></span></a><strong>Understanding D columns</strong>
</h2>

<p><code class="language-plaintext highlighter-rouge">D1</code> column is known from the discussions to be “days since the client
credit card began”.  Subtracting this value from the “Transaction Day”
will result in CONSTANT values per client. <code class="language-plaintext highlighter-rouge">D1n</code> is the created column
after subtraction.</p>

<p>We do the same for all <code class="language-plaintext highlighter-rouge">D</code> columns irrespective, and allow the model
to decide what is important and what is not. Following is an example
of an “assumed client-variable set”. Notice how the <code class="language-plaintext highlighter-rouge">D1n</code> columns are
constant and the <code class="language-plaintext highlighter-rouge">D1</code> columns increase over time, when the data is
sorted on <code class="language-plaintext highlighter-rouge">TransactionDT</code>.</p>

<p><img src="./images/fraud-detection/D1-D1n.png" alt="D1-D1n.png"></p>

<p>Another column <code class="language-plaintext highlighter-rouge">D9</code> denotes the hours at which the transactions are
done. This is tested using <code class="language-plaintext highlighter-rouge">df["Hr"] =
df["TransactionDT"]/(60*60)%24//1/24</code>. The following plot clearly
shows it’s relation to determining if a transaction is Fraud or not.</p>

<p><img src="./images/fraud-detection/D9.png" alt="D9.png"></p>

<h2 id="unbalanced-data">
<a class="anchor" href="#unbalanced-data" aria-hidden="true"><span class="octicon octicon-link"></span></a><strong>Unbalanced data</strong>
</h2>

<p>The Data has only 3.7% transactions denoted as “Fraud”
(<code class="language-plaintext highlighter-rouge">isFraud</code>=1). <code class="language-plaintext highlighter-rouge">RandomOverSampling</code> and <code class="language-plaintext highlighter-rouge">Smote</code> didn’t do much for the
score so I didn’t keep it.</p>

<h2 id="eda-for-uid">
<a class="anchor" href="#eda-for-uid" aria-hidden="true"><span class="octicon octicon-link"></span></a><strong>EDA for UID</strong>
</h2>

<p>As said before this competition is not just about predicting
fraudulent transactions over time, it’s about predicting clients who
are more likely to have <code class="language-plaintext highlighter-rouge">isFraud</code> transactions. <em>Client account, client
email addresses and client address</em> associated with a Fraudulent
transactions is made <code class="language-plaintext highlighter-rouge">isFraud==1</code> <em>in a posterior fashion</em>. We need to
thus find these client variables (UIDs) to be able to predict better.</p>

<p><strong>Finding the UIDs</strong></p>

<p>I stood on the <a href="https://www.kaggle.com/c/ieee-fraud-detection/discussion/111510">shoulders of giants</a>, to find help on this part.</p>

<p>UIDs are nothing but variables that help identify a particular
clients’ transactions. Not all transactions are fraud for a client but
“most” are expected to be. This allows us to determine the quality of
a UID.</p>

<p>To find the UID, a bit of guess and <strong>adversarial validation</strong> (AV) is
used (AV is explained later). Based on how “isFraud” is defined we can
already guess that <code class="language-plaintext highlighter-rouge">card1</code>, <code class="language-plaintext highlighter-rouge">addr1</code> &amp; <code class="language-plaintext highlighter-rouge">P_emaildomain</code> are probably
part of UID. In addition we use AV to determine the rest of the
columns.</p>

<p><img src="./images/fraud-detection/fi.png" alt="fi.png"></p>

<p>To evaluate how good the UID is, we look at how much percent of the
clients have both <code class="language-plaintext highlighter-rouge">isFraud==0</code> and <code class="language-plaintext highlighter-rouge">isFraud==1</code> and we check manually
if they happen one after the other in sequential manner. Shown below
are the evaluations for different UIDs.</p>

<table>
  <thead>
    <tr>
      <th> </th>
      <th>Mixed</th>
      <th>isFraud==1</th>
      <th>isFraud==0</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>uid0</td>
      <td>1.9%</td>
      <td>1.9%</td>
      <td>96%</td>
    </tr>
    <tr>
      <td>uid1</td>
      <td>1.4%</td>
      <td>2.3%</td>
      <td>96.3%</td>
    </tr>
    <tr>
      <td>uid2</td>
      <td>0.79%</td>
      <td>2.6%</td>
      <td>96.5%</td>
    </tr>
    <tr>
      <td>uid3</td>
      <td>0.43%</td>
      <td>2.6%</td>
      <td>96.9%</td>
    </tr>
    <tr>
      <td>uid4</td>
      <td>0.38</td>
      <td>2.6%</td>
      <td>97%</td>
    </tr>
  </tbody>
</table>

<p>UID0: D10n, card1, addr1<br>
UID1: D1n, card1, addr1<br>
UID2: D1n,card1,addr1,p_emaildomain<br>
UID3: D1n,card1,addr1,p_emaildomain, D3n<br>
UID4: D1n,card1,addr1,p_emaildomain, D3n, V1, M7</p>

<p>In all cases “many” of the “Mixed” clients don’t have <code class="language-plaintext highlighter-rouge">isFraud==0</code> and
<code class="language-plaintext highlighter-rouge">isFraud==1</code> sequentially (as shown below). So it looks like most of
the Mixed clients are wrongly classified. I tried refining it with
several other combinations but had little success. So I moved on to
the testing out how these UIDs actually helped.</p>

<p><img src="./images/fraud-detection/mixed-isFraud.png" alt="mixed-isFraud.png"></p>

<p><strong>Final score with different UIDs (UID2 is the best)</strong></p>

<table>
  <thead>
    <tr>
      <th> </th>
      <th>public LB</th>
      <th> </th>
      <th>Private LB</th>
      <th> </th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>uid1</td>
      <td>0.9534</td>
      <td>-</td>
      <td>0.9220</td>
      <td>-</td>
    </tr>
    <tr>
      <td>uid2</td>
      <td>0.954</td>
      <td>+0.0006</td>
      <td>0.925</td>
      <td>+0.003</td>
    </tr>
    <tr>
      <td>uid3</td>
      <td>0.9464</td>
      <td>-0.007</td>
      <td>0.913</td>
      <td>-0.012</td>
    </tr>
  </tbody>
</table>

<hr>

<h1 id="model">
<a class="anchor" href="#model" aria-hidden="true"><span class="octicon octicon-link"></span></a>Model</h1>

<p>I started off with Random Forests, but didn’t get far with it. XGB,
Catboost and LGBM seem to be preferred decision trees models in the ML
community. So I chose XGB and stuck with it till the end to see what
are the best results I can get with it. I used the default values and
that was just good enough.</p>

<p>My classifier was made of:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    <span class="n">clf</span> <span class="o">=</span> <span class="n">xgb</span><span class="p">.</span><span class="n">XGBClassifier</span><span class="p">(</span><span class="n">n_estimators</span><span class="o">=</span><span class="mi">2000</span><span class="p">,</span> 
                        <span class="n">max_depth</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> 
                        <span class="n">learning_rate</span><span class="o">=</span> <span class="mf">0.05</span><span class="p">,</span>
                        <span class="n">subsample</span><span class="o">=</span> <span class="mf">0.6</span><span class="p">,</span>
                        <span class="n">colsample_bytree</span><span class="o">=</span> <span class="mf">0.4</span><span class="p">,</span> 
                        <span class="n">random_state</span> <span class="o">=</span> <span class="n">r</span><span class="p">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">9999</span><span class="p">),</span>
                        <span class="n">use_label_encoder</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span>
                        <span class="n">tree_method</span><span class="o">=</span><span class="s">'gpu_hist'</span><span class="p">)</span>

</code></pre></div></div>

<p>As I wanted to get the max score possible, I used <code class="language-plaintext highlighter-rouge">random_state =
r.randint(0,9999)</code>, as this would not keep the random_state
constant. Simulations in this manner produced variation in the order
of <code class="language-plaintext highlighter-rouge">+-0.002</code> AUC score in some cases. So when it is unclear if a
hypothesis works or not, I run the same simulation a few times and
take its average.</p>

<p>Ensembling is also an option but I haven’t tried it in this
competition. Based on the top solutions they seem to improve the score
too.</p>

<h1 id="feature-engineering">
<a class="anchor" href="#feature-engineering" aria-hidden="true"><span class="octicon octicon-link"></span></a>Feature engineering</h1>

<ol>
  <li>
    <p><strong>Fillna</strong></p>

    <p><code class="language-plaintext highlighter-rouge">df.fillna(-9999,inplace=True)</code></p>

    <p>XGB is capable of handling NaNs. It places the NaN rows in one of
the splits at each node, (based on which gives a better impurity
score). However if we choose a number to represent NaNs then it
treats the NaNs like just another value/category. And it is a bit
faster, due to lesser computations.</p>

    <p>However <code class="language-plaintext highlighter-rouge">fillna</code> boosts the score only by <code class="language-plaintext highlighter-rouge">0.0002</code>.</p>
  </li>
  <li>
    <p><strong>Label Encoding of Categorical data</strong></p>

    <p>All columns with &lt;32000 unique values are made integers and label
 encoded. This is done to reduce the ram usage. A string in python
 uses almost <a href="https://stackoverflow.com/q/58041687/5986651">twice as much memory</a> as an integer. Label
 encoding is done using:</p>

    <p><code class="language-plaintext highlighter-rouge">df.factorize()</code></p>
  </li>
  <li>
    <p><strong>One hot encoding the NaN structure for certain columns</strong></p>

    <p><code class="language-plaintext highlighter-rouge">D2</code> has 50% NaNs and is highly correlated (<code class="language-plaintext highlighter-rouge">0.98</code>) with <code class="language-plaintext highlighter-rouge">D1</code>. In
 this case <code class="language-plaintext highlighter-rouge">D2</code> is removed and the NaN structure is alone kept.</p>

    <p><code class="language-plaintext highlighter-rouge">D9</code> columns represents the time of transaction in a day. But it
 contains 86% NaN values. So a new <code class="language-plaintext highlighter-rouge">D9</code> column (<code class="language-plaintext highlighter-rouge">HrOfDay</code>) was
 created from <code class="language-plaintext highlighter-rouge">TransactionDT</code> using <code class="language-plaintext highlighter-rouge">df["Hr"] =
 df["TransactionDT"]/(60*60)%24//1/24</code>. And the NaN structure of
 <code class="language-plaintext highlighter-rouge">D9</code> is One Hot Encoded.</p>
  </li>
  <li>
    <p><strong>Splitting</strong></p>

    <p>There are many categorical columns, that would allow for better
 models when split. For example, we have “TransactionAmt”, which
 allows for the split: “dollars” and “cents”. <code class="language-plaintext highlighter-rouge">cents</code> could be a
 proxy for identifying if the transaction is from another country
 than US. Possibly there could be a pattern on how frauds happen
 with “cents”. The “split” is done using the following function:</p>

    <div class="language-python highlighter-rouge">
<div class="highlight"><pre class="highlight"><code> <span class="k">def</span> <span class="nf">split_cols</span><span class="p">(</span><span class="n">df</span><span class="p">,</span> <span class="n">col</span><span class="p">):</span>
 <span class="n">df</span><span class="p">[</span><span class="s">'cents'</span><span class="p">]</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="n">col</span><span class="p">].</span><span class="n">mod</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
 <span class="n">df</span><span class="p">[</span><span class="n">col</span><span class="p">]</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="n">col</span><span class="p">].</span><span class="n">floordiv</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
</code></pre></div>    </div>

    <p><img src="./images/fraud-detection/cents.png" alt="cents.png"></p>

    <p>Another example is <code class="language-plaintext highlighter-rouge">id_31</code>. It has values such as <code class="language-plaintext highlighter-rouge">chrome 65.0</code>,
 <code class="language-plaintext highlighter-rouge">chrome 66.0 for android</code>. To aid the model we split the version
 number and browser using the commands below. The same has been
 done for several other columns, and kept if it resulted in an
 increase in score or featured high in the feature importance
 plots.</p>

    <div class="language-python highlighter-rouge">
<div class="highlight"><pre class="highlight"><code> <span class="n">lst_to_rep</span> <span class="o">=</span> <span class="p">[</span><span class="s">r"^.*chrome.*$"</span><span class="p">,</span><span class="s">r"^.*aol.*$"</span><span class="p">,</span><span class="s">r"^.*[Ff]irefox.*$"</span><span class="p">...]</span>

 <span class="n">lst_val</span> <span class="o">=</span> <span class="p">[</span><span class="s">"chrome"</span><span class="p">,</span><span class="s">"aol"</span><span class="p">,</span><span class="s">"firefox"</span><span class="p">,</span><span class="s">"google"</span><span class="p">,</span><span class="s">"ie"</span><span class="p">,</span><span class="s">"safari"</span><span class="p">,</span><span class="s">"opera"</span><span class="p">,</span><span class="s">"samsung"</span><span class="p">,</span><span class="s">"edge"</span><span class="p">,</span><span class="s">"chrome"</span><span class="p">]</span>
 <span class="n">df</span><span class="p">[</span><span class="s">"id_31_browser"</span><span class="p">].</span><span class="n">replace</span><span class="p">(</span><span class="n">to_replace</span><span class="o">=</span><span class="n">lst_to_rep</span><span class="p">,</span> <span class="n">value</span><span class="o">=</span><span class="n">lst_val</span><span class="p">,</span> <span class="n">regex</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span><span class="n">inplace</span><span class="o">=</span><span class="bp">True</span><span class="p">);</span>
</code></pre></div>    </div>

    <p><img src="./images/fraud-detection/id_31.png" alt="id_31.png"></p>
  </li>
  <li>
    <p><strong>Combining</strong></p>

    <p>Combining values such as <code class="language-plaintext highlighter-rouge">card1</code> and <code class="language-plaintext highlighter-rouge">addr1</code>, by themselves they
 might not mean much, but together they could correlate to
 something meaningful. One such combination is the UID. But we
 don’t keep the UID just as we don’t keep the time columns.</p>
  </li>
  <li>
    <p><strong>Frequency encoding</strong></p>

    <p>The frequency of the values of a column seems important to detect
 if a transaction is fraud or not.</p>

    <div class="language-python highlighter-rouge">
<div class="highlight"><pre class="highlight"><code> <span class="k">def</span> <span class="nf">encode_CB2</span><span class="p">(</span><span class="n">df1</span><span class="p">,</span><span class="n">uid</span><span class="p">):</span>
     <span class="n">newcol</span> <span class="o">=</span> <span class="s">"_"</span><span class="p">.</span><span class="n">join</span><span class="p">(</span><span class="n">uid</span><span class="p">)</span>
     <span class="c1">## make combined column
</span>     <span class="n">df1</span><span class="p">[</span><span class="n">newcol</span><span class="p">]</span> <span class="o">=</span> <span class="n">df1</span><span class="p">[</span><span class="n">uid</span><span class="p">].</span><span class="n">astype</span><span class="p">(</span><span class="nb">str</span><span class="p">).</span><span class="nb">apply</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="s">'_'</span><span class="p">.</span><span class="n">join</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</code></pre></div>    </div>
    <p>Added several features based on this and resulted in increase in
 score (documented below).</p>
  </li>
  <li>
    <p><strong>Aggregation (transforms) while imputing NaNs</strong></p>

    <p>This is one of the most important parts of the solution which
 boosted the score all the way into top 10% from top 30%.  Why
 Aggregations work is explained <a href="https://www.kaggle.com/c/ieee-fraud-detection/discussion/111453">here</a>. The aggregation is done
 after combining the train and test dataframes. The following
 <code class="language-plaintext highlighter-rouge">groupby</code> command does it all.</p>

    <p><code class="language-plaintext highlighter-rouge">df_all.groupby(uid,dropna=False)["TransactionAmt"].transform("mean").reset_index(drop=True)</code></p>

    <p>It is very important to add <code class="language-plaintext highlighter-rouge">dropna=False</code>, as there are many NaN
 rows which would be dropped otherwise. <code class="language-plaintext highlighter-rouge">fillna</code> is not done until
 the aggregations are made. This way, Nan’s in the aggregated
 column get imputed.</p>

    <p>Finding the columns to be aggregated was possible using just the
 AV feature importance seen above and a bit of logic.</p>
  </li>
  <li>
    <p><strong>Removing redundant columns based on Feature Importance</strong></p>

    <p>As far as I have seen, removing redundant columns makes the model
 faster and rarely improves the score. Having said that I tried to
 remove columns that seemed redundant but got the score reduced by
 0.002, which is a LOT in this competition. So I kept all those
 variables. However removing redundant <code class="language-plaintext highlighter-rouge">V</code> columns (200 of them)
 gives a large decrease in time of computation. So those are the
 ones that are removed.</p>
  </li>
  <li>
    <p><strong>Removing time columns</strong> such as <code class="language-plaintext highlighter-rouge">TransactionDT</code> and <code class="language-plaintext highlighter-rouge">TransactionID</code></p>
  </li>
</ol>

<p><img src="./images/fraud-detection/TransactionID.png" alt="TransactionID.png"></p>

<p><strong>What worked and by how much</strong></p>

<table>
  <thead>
    <tr>
      <th>Method</th>
      <th>Public LB</th>
      <th> </th>
      <th>Private LB</th>
      <th> </th>
      <th>Percentile</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>baseline</td>
      <td>0.9384</td>
      <td> </td>
      <td>0.9096</td>
      <td> </td>
      <td>Top 80%</td>
    </tr>
    <tr>
      <td>remove 200 <code class="language-plaintext highlighter-rouge">V</code>
</td>
      <td>0.9377</td>
      <td>-0.003</td>
      <td>0.9107</td>
      <td>+0.001</td>
      <td>Top 80%</td>
    </tr>
    <tr>
      <td>remove time cols</td>
      <td>0.9374</td>
      <td>-0.0003</td>
      <td>0.9109</td>
      <td>+0.0002</td>
      <td>Top 80%</td>
    </tr>
    <tr>
      <td>Transform <code class="language-plaintext highlighter-rouge">D</code>
</td>
      <td>0.9429</td>
      <td>+0.0055</td>
      <td>0.9117</td>
      <td>+0.0008</td>
      <td>top 50%</td>
    </tr>
    <tr>
      <td>Combine and FE</td>
      <td>0.9471</td>
      <td>+0.0042</td>
      <td>0.9146</td>
      <td>+0.0029</td>
      <td>top 30%</td>
    </tr>
    <tr>
      <td>Agg on uid1</td>
      <td>0.9513</td>
      <td>+0.0042</td>
      <td>0.9203</td>
      <td>+0.0057</td>
      <td>top 20%</td>
    </tr>
    <tr>
      <td>additional agg</td>
      <td>0.9535</td>
      <td>+0.0022</td>
      <td>0.9220</td>
      <td>+0.0017</td>
      <td>top 10%</td>
    </tr>
    <tr>
      <td>fillna</td>
      <td>0.9537</td>
      <td>+0.0002</td>
      <td>0.9223</td>
      <td>+0.0003</td>
      <td>top 10%</td>
    </tr>
  </tbody>
</table>

<h1 id="post-process">
<a class="anchor" href="#post-process" aria-hidden="true"><span class="octicon octicon-link"></span></a>Post Process</h1>

<p><code class="language-plaintext highlighter-rouge">isFraud</code> of the same clients are all expected to be <code class="language-plaintext highlighter-rouge">0</code> or <code class="language-plaintext highlighter-rouge">1</code>. So,
tried averaging the “identified-client’s” probabilities across train
and test. This resulted in a poorer score by upto <code class="language-plaintext highlighter-rouge">0.003</code>. So didn’t
end up using it.</p>

<h1 id="local-validation-and-prediction-strategy">
<a class="anchor" href="#local-validation-and-prediction-strategy" aria-hidden="true"><span class="octicon octicon-link"></span></a>Local Validation and Prediction Strategy</h1>

<p>Initially I started with a 10% sample dataset and looked at a hold
out. Considering that it took 7 mins per simulation for the entire
dataset, I went on to use the entire dataset instead of just the
sample.</p>

<p>I tried a couple of strategies here and compared to test solution:</p>

<ol>
  <li>Holdout and predict average of 5 models</li>
  <li>5Fold CV unshuffled.</li>
  <li>Group 5fold CV based on Month</li>
</ol>

<p>The following are the results:</p>

<table>
  <thead>
    <tr>
      <th> </th>
      <th>Local</th>
      <th>Public LB</th>
      <th>Private LB</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>5Fold CV unshuffled</td>
      <td>baseline</td>
      <td>baseline</td>
      <td>baseline</td>
    </tr>
    <tr>
      <td>Holdout (5x)</td>
      <td>-0.01</td>
      <td>-0.013</td>
      <td>-0.016</td>
    </tr>
    <tr>
      <td>Month 5fold CV</td>
      <td>+0.0022</td>
      <td>-0.0006</td>
      <td>+0.0000</td>
    </tr>
  </tbody>
</table>

<p><code class="language-plaintext highlighter-rouge">The Holdout method</code> seems obviously not good. <code class="language-plaintext highlighter-rouge">Month 5fold CV</code> did not
produce significant increases (&gt;0.002) in LB so I skip it. All are
compared to the baseline values (<code class="language-plaintext highlighter-rouge">5fold CV unshuffled</code>).</p>

<p><strong>Note</strong>: As this is a time series simulation people had warned about
not using CV: <a href="https://www.kaggle.com/c/ieee-fraud-detection/discussion/99993#577626">here</a>, and <a href="https://www.kaggle.com/c/ieee-fraud-detection/discussion/99993#576742">here</a>. However the best results,
including that of the top solution for this competition were only
possible due to CV.</p>

<h1 id="memory-reduction">
<a class="anchor" href="#memory-reduction" aria-hidden="true"><span class="octicon octicon-link"></span></a>Memory reduction</h1>

<ol>
  <li>
    <p>Label encoding of categorical variables.</p>
  </li>
  <li>
    <p>Converting variables to float32 should be <a href="https://stackoverflow.com/a/64069641/5986651">good enough</a>.</p>

    <p>16bit: 0.1235
 32bit: 0.12345679
 64bit: 0.12345678912121212</p>

    <p>With the AUC numbers, 8 digits beyond the decimal seems good enough.</p>
  </li>
  <li>
    <p>Removing 200 V columns out of the 439 columns with little change in
score allowed to finally do AV on the kaggle kernel without going
out of memory.</p>
  </li>
</ol>

<h1 id="practical-understanding-of-overfitting-and-av">
<a class="anchor" href="#practical-understanding-of-overfitting-and-av" aria-hidden="true"><span class="octicon octicon-link"></span></a>Practical understanding of Overfitting and AV</h1>

<h2 id="preventing-overfitting">
<a class="anchor" href="#preventing-overfitting" aria-hidden="true"><span class="octicon octicon-link"></span></a><strong>Preventing overfitting</strong>
</h2>

<p>There are usually two reasons why the Training score and Test score
differ:</p>

<ol>
  <li>
    <p>Overfitting</p>
  </li>
  <li>
    <p>Out of Domain Data</p>

    <p>i.e., test and train data are from different times, or different
 clients etc…</p>
  </li>
</ol>

<p>There is a nice trick to see what is causing the difference in scores
between the training and the test data: Determining OOF (out-of-fold)
scores. The OOF score is basically a score on unseen data but within
the training data domain itself. It nicely controls for the effect of
“out of domain” data. So,</p>

<ul>
  <li>
    <p>OOF (out-of-fold) scores &lt; Train scores ==&gt; <strong>Overfitting</strong></p>
  </li>
  <li>
    <p>Test scores &lt; OOF scores ==&gt; <strong>Out-of-domain data</strong></p>
  </li>
</ul>

<p><strong>Example of only over-fitting</strong></p>

<p>From the beginning I have suffered mainly with overfitting rather than
the out-of-domain data issue. My <code class="language-plaintext highlighter-rouge">OOF&lt;&lt;Train</code> (indicating overfitting)
and <code class="language-plaintext highlighter-rouge">Test&gt;OOF</code> (not indicating “out-of-domain” issue):</p>

<table>
  <thead>
    <tr>
      <th> </th>
      <th>train</th>
      <th>OOF</th>
      <th>test (public)</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>baseline AUC</td>
      <td>0.99</td>
      <td>0.9292</td>
      <td>0.9384</td>
    </tr>
  </tbody>
</table>

<p><strong>Example of both over-fitting and out of domain issue</strong></p>

<p>In another case, where I accidentally changed some values of columns
in the test dataset as NaNs, I saw the following. Here <code class="language-plaintext highlighter-rouge">OOF&lt;&lt;Train</code>
(indicating overfitting) and also <code class="language-plaintext highlighter-rouge">Test&lt;&lt;OOF</code> (indicating
“out-of-domain” issue):</p>

<table>
  <thead>
    <tr>
      <th> </th>
      <th>train</th>
      <th>OOF</th>
      <th>test (public)</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>AUC</td>
      <td>0.9971</td>
      <td>0.9426</td>
      <td>0.9043</td>
    </tr>
  </tbody>
</table>

<p>Looking at the important AV columns, and probing into those columns
allowed me to fix the issue.</p>

<p><strong>Preventing overfitting</strong></p>

<p>What appears to reduce overfitting are:</p>

<ol>
  <li>
    <p>removal of time and client columns such as <code class="language-plaintext highlighter-rouge">TransactionID</code> and
<code class="language-plaintext highlighter-rouge">TransactionDT</code>, Created UIDs etc…</p>
  </li>
  <li>
    <p>Other Feature Engineering based columns (Frequency Encoding,
combining columns, and most important of all Aggregations).</p>
  </li>
  <li>
    <p>Choosing the right parameters for the XGB model (i.e., depth of
tree etc…).</p>
  </li>
</ol>

<h2 id="what-can-your-av-do-for-you">
<a class="anchor" href="#what-can-your-av-do-for-you" aria-hidden="true"><span class="octicon octicon-link"></span></a><strong>What can your AV do for you?</strong>
</h2>

<p>Adverserial Validation is a simple technique that helps distinguish
the difference in the train and the test data. In <a href="https://www.kaggle.com/thejravichandran/adverserial-validation-where-auc-1">this kernel</a>, I
show how to do AV with a simple example. It involves the following
steps:</p>

<ol>
  <li>Concat the train and the test data set.</li>
  <li>Append a new column “is_test”.</li>
  <li>Split data into training and validation.</li>
  <li>Train model and get AUC score on <code class="language-plaintext highlighter-rouge">istest==1</code> for the validating set.</li>
</ol>

<p><strong>AV can identify out-of-domain data</strong></p>

<p><img src="./images/fraud-detection/av-overfitting.png" alt="av-overfitting.png"></p>

<p>In the image above, the circles (blue) denote training data. The blue
line denotes the line fit. Red line denotes an overfit line. Different
colored stars (blue, green, red, yellow) denote different types of
test data. <code class="language-plaintext highlighter-rouge">Y</code> is the dependent variable and <code class="language-plaintext highlighter-rouge">X</code> is the independent
variable. The dotted blue line indicates the predicted value in case
the data lies beyond the training bounds.</p>

<p>The three regions marked in “big bubbles” are where there is no
training data and hence these are <strong>out-of-domain data regions</strong>. We
simply get a very high AUC score (<code class="language-plaintext highlighter-rouge">AUC=1</code>) from AV for these (red,
green and yellow stars). In <a href="https://www.kaggle.com/thejravichandran/adverserial-validation-where-auc-1">this kernel</a> it is checked with an
example that the “big bubbles” in the image above have AV <code class="language-plaintext highlighter-rouge">AUC=1</code>.</p>

<p><strong>AV <code class="language-plaintext highlighter-rouge">AUC=1</code> and how they affect test predictions</strong></p>

<p>When the test set is denoted by the green stars, it is clear that the
resulting test score is going to be “bad”. However, when the test set
is denoted by the yellow stars, error in predicting seems to be less
in comparison (despite having AV <code class="language-plaintext highlighter-rouge">AUC=1</code>). When the test set is
denoted by the red stars the error doesn’t seem to be that bad either
AV <code class="language-plaintext highlighter-rouge">AUC=1</code>.</p>

<p>From the beginning to the end for this competition, I had <code class="language-plaintext highlighter-rouge">&gt;0.9 AUC</code>,
and nevertheless ended up with very good results (0.953–&gt;top 10%). In
addition the OOF score (from training) was less than the TEST score
informing that the OUT-OF-DOMAIN data was not the problem for the
score. I am thus inclined to think that in my case I probably end up
with test data which are out of domain like the red and yellow stars
and not like the green stars.</p>

<p><strong>How AV is used in this kaggle competition</strong></p>

<ol>
  <li>
    <p>Identify and Remove time columns like <code class="language-plaintext highlighter-rouge">TransactionID</code> and
<code class="language-plaintext highlighter-rouge">TransactionDT</code></p>

    <p>When AV is first run on this dataset, two columns standout:
<code class="language-plaintext highlighter-rouge">TransactionDT</code> and <code class="language-plaintext highlighter-rouge">TransactionID</code>. One is the time info in
seconds and the other is the id of each transaction. Ideally we
don’t want to be using such time columns as we don’t want the model
to learn anything specific to the Date or the ID of
transactions. AV provides a platform to identify such variables and
eventually we can get rid of them.</p>
  </li>
  <li>
    <p>Removing very different columns for negligible score loss.</p>

    <p>In one of the iterations I had 203 columns with an lb score of
 <code class="language-plaintext highlighter-rouge">0.953</code>. Removing 20 of the most important AV columns resulted in
 a small decrease in score <code class="language-plaintext highlighter-rouge">0.9511</code>. I always try to see how the
 “out of domain” data affects our result.</p>
  </li>
  <li>
    <p>AV helps find aggregations that we need</p>

    <p>This is and will be the greatest reason for doing AV. AV is so
 powerful that improving my score from top 80% to a top 10% score
 was done purely by looking at the important AV columns.</p>

    <p>I pretty much used the first 10-20 important AV columns (and a few
 columns on my own) to determine which columns to choose as UID and
 which to aggregate over.</p>
  </li>
  <li>
    <p>Find mistakes with your AV</p>

    <p>I applied aggregation to the training dataset and accidentally did
 not apply it to the test. This was promptly visible in the AV as
 the aggregated columns showed up first in the “AV important
 columns”. A quick look at the top AV important columns and I found
 my error.</p>

    <p>For example, in one of the experiments I got the following:</p>

    <table>
      <thead>
        <tr>
          <th> </th>
          <th>train</th>
          <th>OOF</th>
          <th>test (public)</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td>AUC</td>
          <td>0.9971</td>
          <td>0.9426</td>
          <td>0.9043</td>
        </tr>
      </tbody>
    </table>

    <p>From this it is possible to infer that there is both overfitting
 (train»OOF) and Out of Domain issue (OOF»test). Using AV, I was
 able to find out which columns were giving me the problem. When I
 probed in deeper into the columns, it turned out that I had
 accidentally added NaN’s to the test data.</p>

    <p>Once I corrected for it, I ended up with:</p>

    <table>
      <thead>
        <tr>
          <th> </th>
          <th>train</th>
          <th>OOF</th>
          <th>test (public)</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td>AUC</td>
          <td>0.998</td>
          <td>0.9474</td>
          <td>0.9530</td>
        </tr>
      </tbody>
    </table>
  </li>
</ol>

<h2 id="references">
<a class="anchor" href="#references" aria-hidden="true"><span class="octicon octicon-link"></span></a>References</h2>

<ol>
  <li><a href="https://www.kaggle.com/c/ieee-fraud-detection/discussion/101203">Data description</a></li>
  <li><a href="https://www.kaggle.com/alijs1/ieee-transaction-columns-reference">Plots and much more for many features</a></li>
  <li>
<a href="https://www.kaggle.com/c/ieee-fraud-detection/discussion/111284">Top Solution </a>, <a href="https://www.kaggle.com/cdeotte/xgb-fraud-with-magic-0-9600/notebook#The-Magic-Feature---UID">top solution model</a>
</li>
  <li><a href="https://www.kaggle.com/c/ieee-fraud-detection/discussion/111453">How the Magic UID solution Works</a></li>
  <li><a href="https://www.kaggle.com/c/ieee-fraud-detection/discussion/111320">Other ideas to find UIDs</a></li>
  <li><a href="https://www.kaggle.com/c/ieee-fraud-detection/discussion/108575">Notes on Feature Engineering</a></li>
  <li><a href="https://www.kaggle.com/c/ieee-fraud-detection/discussion/107697">Lessons learnt from Top solution</a></li>
  <li>
<a href="https://www.kaggle.com/nroman/eda-for-cis-fraud-detection#ProductCD">Other nice EDAs</a>, and <a href="https://www.kaggle.com/jesucristo/fraud-complete-eda#Memory-reduction">here</a>
</li>
  <li>17th place solution</li>
  <li><a href="https://www.kaggle.com/akasyanama13/eda-what-s-behind-d-features">How to investigate D features</a></li>
  <li><a href="https://www.kaggle.com/c/ieee-fraud-detection/discussion/99993#576742">Don’t use Time features</a></li>
  <li><a href="https://github.com/fastai/fastbook/blob/master/09_tabular.ipynb">Fastai tabular NN</a></li>
</ol>


  </div><!-- from https://github.com/utterance/utterances -->
<script src="https://utteranc.es/client.js"
        repo="tkravichandran/my-fast-ds-blog"
        issue-term="title"
        label="blogpost-comment"
        theme="github-light"
        crossorigin="anonymous"
        async>
</script><a class="u-url" href="/my-fast-ds-blog/first-tabular-kaggle-competition.html" hidden></a>
</article>
      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/my-fast-ds-blog/"></data>

  <div class="wrapper">

    <div class="footer-col-wrapper">
      <div class="footer-col">
        <p class="feed-subscribe">
          <a href="/my-fast-ds-blog/feed.xml">
            <svg class="svg-icon orange">
              <use xlink:href="/my-fast-ds-blog/assets/minima-social-icons.svg#rss"></use>
            </svg><span>Subscribe</span>
          </a>
        </p>
      </div>
      <div class="footer-col">
        <p></p>
      </div>
    </div>

    <div class="social-links"><ul class="social-media-list"><li><a rel="me" href="https://github.com/tkravichandran" title="tkravichandran"><svg class="svg-icon grey"><use xlink:href="/my-fast-ds-blog/assets/minima-social-icons.svg#github"></use></svg></a></li></ul>
</div>

  </div>

</footer>
</body>

</html>
